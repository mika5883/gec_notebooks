{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd914aa-ff94-451e-afbf-817198cea55d",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7199d569-57bf-481a-af84-2d88b9a4185f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:35:28.982517Z",
     "iopub.status.busy": "2024-12-17T15:35:28.981288Z",
     "iopub.status.idle": "2024-12-17T15:35:39.674955Z",
     "shell.execute_reply": "2024-12-17T15:35:39.673495Z",
     "shell.execute_reply.started": "2024-12-17T15:35:28.982481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip -q install datasets\n",
    "# %pip -q install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fee1c9-78c0-4d67-8afe-da5577beaf51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:35:39.678320Z",
     "iopub.status.busy": "2024-12-17T15:35:39.677576Z",
     "iopub.status.idle": "2024-12-17T15:36:22.592458Z",
     "shell.execute_reply": "2024-12-17T15:36:22.591257Z",
     "shell.execute_reply.started": "2024-12-17T15:35:39.678241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2024-12-17 15:36:06.540714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-17 15:36:11.758844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9598c99-3b4e-4ad4-bbea-76c40fe80737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:22.594619Z",
     "iopub.status.busy": "2024-12-17T15:36:22.593644Z",
     "iopub.status.idle": "2024-12-17T15:36:22.618189Z",
     "shell.execute_reply": "2024-12-17T15:36:22.617044Z",
     "shell.execute_reply.started": "2024-12-17T15:36:22.594555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b73f8-e2d2-4a7c-9cde-4dbcfd7a2520",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e5cee2-015f-4cb5-9be5-a6c0cc01d942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:22.621137Z",
     "iopub.status.busy": "2024-12-17T15:36:22.620399Z",
     "iopub.status.idle": "2024-12-17T15:36:22.816396Z",
     "shell.execute_reply": "2024-12-17T15:36:22.815297Z",
     "shell.execute_reply.started": "2024-12-17T15:36:22.621086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dev = '/home/jupyter/datasphere/project/rugec/data/RULEC-GEC.dev.tsv'\n",
    "train = '/home/jupyter/datasphere/project/rugec/data/RULEC-GEC.train.tsv'\n",
    "test = '/home/jupyter/datasphere/project/rugec/data/RULEC-GEC.test.tsv'\n",
    "\n",
    "rulec_train = pd.read_csv(train, delimiter='\\t')\n",
    "rulec_test = pd.read_csv(test, delimiter='\\t')\n",
    "rulec_dev = pd.read_csv(dev, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b56099-e9bf-47d6-ba36-de0b52be8488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:22.818472Z",
     "iopub.status.busy": "2024-12-17T15:36:22.817494Z",
     "iopub.status.idle": "2024-12-17T15:36:22.861891Z",
     "shell.execute_reply": "2024-12-17T15:36:22.860552Z",
     "shell.execute_reply.started": "2024-12-17T15:36:22.818420Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrupt_sent</th>\n",
       "      <th>correct_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Экскурсия прошла великолепно , Владимир Анатол...</td>\n",
       "      <td>Экскурсия прошла великолепно , Владимир Анатол...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Курсовая может быть о любой теме , которую обс...</td>\n",
       "      <td>Курсовая может быть по любой теме , которую об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Я вижу , я слышу всё вокруг меня .</td>\n",
       "      <td>Я вижу , я слышу всё вокруг меня .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Служить в Российской армии ( для мужчин )</td>\n",
       "      <td>Служить в Российской армии ( для мужчин )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Вряд ли все мир обеспокоился бы из-за бунта в ...</td>\n",
       "      <td>Вряд ли весь мир обеспокоился бы из-за бунта в...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        corrupt_sent                                       correct_sent\n",
       "0  Экскурсия прошла великолепно , Владимир Анатол...  Экскурсия прошла великолепно , Владимир Анатол...\n",
       "1  Курсовая может быть о любой теме , которую обс...  Курсовая может быть по любой теме , которую об...\n",
       "2                 Я вижу , я слышу всё вокруг меня .                 Я вижу , я слышу всё вокруг меня .\n",
       "3          Служить в Российской армии ( для мужчин )          Служить в Российской армии ( для мужчин )\n",
       "4  Вряд ли все мир обеспокоился бы из-за бунта в ...  Вряд ли весь мир обеспокоился бы из-за бунта в..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rulec_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82111d5-6e87-4a63-8444-d84a029f8b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:22.864864Z",
     "iopub.status.busy": "2024-12-17T15:36:22.863609Z",
     "iopub.status.idle": "2024-12-17T15:36:22.983690Z",
     "shell.execute_reply": "2024-12-17T15:36:22.982444Z",
     "shell.execute_reply.started": "2024-12-17T15:36:22.864770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fine_tune = {'train':Dataset.from_dict({'corrupt_sent':rulec_train['corrupt_sent'],'correct_sent' : rulec_train['correct_sent']}),\n",
    "     'test':Dataset.from_dict({'corrupt_sent':rulec_test['corrupt_sent'],'correct_sent' : rulec_test['correct_sent']}),\n",
    "      'dev':Dataset.from_dict({'corrupt_sent' : rulec_dev['corrupt_sent'], 'correct_sent':rulec_dev['correct_sent']})\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddaecb4-9a54-4976-844a-8b65aec864dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:22.985988Z",
     "iopub.status.busy": "2024-12-17T15:36:22.984907Z",
     "iopub.status.idle": "2024-12-17T15:36:23.013483Z",
     "shell.execute_reply": "2024-12-17T15:36:23.012291Z",
     "shell.execute_reply.started": "2024-12-17T15:36:22.985937Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrupt_sent': 'В книге излагаются основы понимания обучения инностаного языка через разных теорий лингвистов в этом жанре .', 'correct_sent': 'В книге излагаются основы понимания обучения иностранному языку через разные теории лингвистов в этом жанре .'}\n"
     ]
    }
   ],
   "source": [
    "for each in fine_tune['train'].take(1):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b5cb5-022b-41ed-b033-26de2dd07d1e",
   "metadata": {},
   "source": [
    "# Model and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb98309-2a1c-4524-9dbb-32d642f4445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = json.load(open('/home/jupyter/datasphere/project/tokens.json'))\n",
    "tok = CONFIG[\"HF_TOK\"]\n",
    "wdb_tok = CONFIG['WANDB_API_KEY']\n",
    "\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "login(token=tok)\n",
    "wandb.login(key=wdb_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57748f8e-3d9c-4c49-becb-33cc6b5cf98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:23.015991Z",
     "iopub.status.busy": "2024-12-17T15:36:23.014669Z",
     "iopub.status.idle": "2024-12-17T15:36:29.491061Z",
     "shell.execute_reply": "2024-12-17T15:36:29.489866Z",
     "shell.execute_reply.started": "2024-12-17T15:36:23.015936Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('mika5883/pretrain_rugec_msu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('mika5883/pretrain_rugec_msu')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('mika5883/pretrain_rugec_msu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a690e93e-cc1e-4d9e-9ffd-fc05235ff52d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:29.494246Z",
     "iopub.status.busy": "2024-12-17T15:36:29.492497Z",
     "iopub.status.idle": "2024-12-17T15:36:29.515024Z",
     "shell.execute_reply": "2024-12-17T15:36:29.513682Z",
     "shell.execute_reply.started": "2024-12-17T15:36:29.494186Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = [f'grammar: {each}' for each in examples['corrupt_sent']]\n",
    "    targets = [each for each in examples['correct_sent']]\n",
    "\n",
    "    # inputs = [prefix + inp for inp in inputs] #we're skipping this step because our data is prefixed\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=128, padding='max_length', truncation=True)\n",
    "    labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ] #замена токенов паддинга на -100, чтобы они не учитывались при подсчёте потерь\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # model_inputs['corrupt_sent'] = inputs\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdcdf12-fc76-438f-98b1-3a31b3553d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:29.519930Z",
     "iopub.status.busy": "2024-12-17T15:36:29.519008Z",
     "iopub.status.idle": "2024-12-17T15:36:33.423367Z",
     "shell.execute_reply": "2024-12-17T15:36:33.422009Z",
     "shell.execute_reply.started": "2024-12-17T15:36:29.519890Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4980/4980 [00:01<00:00, 3142.28 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3366.48 examples/s]\n",
      "Map: 100%|██████████| 2500/2500 [00:00<00:00, 3323.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train = fine_tune['train'].map(tokenize_function, batched=True)\n",
    "test = fine_tune['test'].map(tokenize_function, batched=True)\n",
    "dev = fine_tune['dev'].map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116868fb-67a7-4878-9bff-0376a786bd17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:33.425442Z",
     "iopub.status.busy": "2024-12-17T15:36:33.424613Z",
     "iopub.status.idle": "2024-12-17T15:36:33.448200Z",
     "shell.execute_reply": "2024-12-17T15:36:33.447086Z",
     "shell.execute_reply.started": "2024-12-17T15:36:33.425389Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corrupt_sent': 'В книге излагаются основы понимания обучения инностаного языка через разных теорий лингвистов в этом жанре .', 'correct_sent': 'В книге излагаются основы понимания обучения иностранному языку через разные теории лингвистов в этом жанре .', 'input_ids': [8, 20849, 13555, 23, 32, 4219, 29, 2540, 310, 11753, 10227, 3887, 5, 8415, 1018, 99, 2855, 226, 1221, 30812, 8, 4819, 6468, 36, 6, 137, 7900, 13, 8, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [32, 4219, 29, 2540, 310, 11753, 10227, 3887, 5, 92, 3637, 457, 18967, 226, 1916, 5428, 8, 4819, 6468, 36, 6, 137, 7900, 13, 8, 4, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "for each in train.take(1):\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e35c1d-f664-46c2-9187-818de88c55ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:36:33.468Z",
     "iopub.status.busy": "2024-12-17T15:36:33.467039Z",
     "iopub.status.idle": "2024-12-17T15:36:33.487616Z",
     "shell.execute_reply": "2024-12-17T15:36:33.486378Z",
     "shell.execute_reply.started": "2024-12-17T15:36:33.467946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='mika5883/pretrain_rugec_msu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5361bcaf-e94f-49cd-8c0b-e2957d7956f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:37:24.584153Z",
     "iopub.status.busy": "2024-12-17T15:37:24.583200Z",
     "iopub.status.idle": "2024-12-17T15:43:19.266736Z",
     "shell.execute_reply": "2024-12-17T15:43:19.265334Z",
     "shell.execute_reply.started": "2024-12-17T15:37:24.584100Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: correct_sent, corrupt_sent. If correct_sent, corrupt_sent are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 4,980\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 234\n",
      "  Number of trainable parameters = 222,903,552\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Tracking run with wandb version 0.18.5\n",
      "wandb: Run data is saved locally in /home/jupyter/work/resources/rugec/notebooks/wandb/run-20241217_153752-dwiydlbx\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run finetune_rugec_msu\n",
      "wandb: ⭐️ View project at https://wandb.ai/mika5883/huggingface\n",
      "wandb: 🚀 View run at https://wandb.ai/mika5883/huggingface/runs/dwiydlbx\n",
      " 33%|███▎      | 78/234 [00:43<01:07,  2.31it/s]The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: correct_sent, corrupt_sent. If correct_sent, corrupt_sent are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 64\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 2/40 [00:00<00:02, 12.83it/s]\u001b[A\n",
      " 10%|█         | 4/40 [00:00<00:04,  7.88it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:00<00:04,  7.28it/s]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:00<00:04,  6.94it/s]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:00<00:04,  6.74it/s]\u001b[A\n",
      " 20%|██        | 8/40 [00:01<00:04,  6.55it/s]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:01<00:04,  6.43it/s]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:01<00:04,  6.39it/s]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:01<00:04,  6.32it/s]\u001b[A\n",
      " 30%|███       | 12/40 [00:01<00:04,  6.32it/s]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:01<00:04,  6.28it/s]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:02<00:04,  6.26it/s]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:02<00:04,  6.24it/s]\u001b[A\n",
      " 40%|████      | 16/40 [00:02<00:03,  6.24it/s]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:02<00:03,  6.18it/s]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:02<00:03,  6.20it/s]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:02<00:03,  6.21it/s]\u001b[A\n",
      " 50%|█████     | 20/40 [00:03<00:03,  6.17it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:03<00:03,  6.21it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:03<00:02,  6.21it/s]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:03<00:02,  6.20it/s]\u001b[A\n",
      " 60%|██████    | 24/40 [00:03<00:02,  6.20it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:03<00:02,  6.23it/s]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:04<00:02,  6.22it/s]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:04<00:02,  6.21it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:04<00:01,  6.19it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:04<00:01,  6.20it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:04<00:01,  6.20it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:04<00:01,  6.22it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:04<00:01,  6.20it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:05<00:01,  6.20it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:05<00:00,  6.18it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:05<00:00,  6.21it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:05<00:00,  6.22it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:05<00:00,  6.22it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:05<00:00,  6.18it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:06<00:00,  6.42it/s]\u001b[A\n",
      "                                                [A\n",
      " 33%|███▎      | 78/234 [00:50<01:07,  2.31it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19952282309532166, 'eval_runtime': 6.3929, 'eval_samples_per_second': 391.056, 'eval_steps_per_second': 6.257, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 156/234 [01:26<00:33,  2.30it/s]AThe following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: correct_sent, corrupt_sent. If correct_sent, corrupt_sent are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 64\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 2/40 [00:00<00:02, 12.78it/s]\u001b[A\n",
      " 10%|█         | 4/40 [00:00<00:04,  7.85it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:00<00:04,  7.29it/s]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:00<00:04,  6.89it/s]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:00<00:04,  6.72it/s]\u001b[A\n",
      " 20%|██        | 8/40 [00:01<00:04,  6.49it/s]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:01<00:04,  6.44it/s]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:01<00:04,  6.38it/s]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:01<00:04,  6.35it/s]\u001b[A\n",
      " 30%|███       | 12/40 [00:01<00:04,  6.26it/s]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:01<00:04,  6.25it/s]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:02<00:04,  6.23it/s]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:02<00:04,  6.21it/s]\u001b[A\n",
      " 40%|████      | 16/40 [00:02<00:03,  6.22it/s]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:02<00:03,  6.20it/s]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:02<00:03,  6.22it/s]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:02<00:03,  6.25it/s]\u001b[A\n",
      " 50%|█████     | 20/40 [00:03<00:03,  6.19it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:03<00:03,  6.23it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:03<00:02,  6.22it/s]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:03<00:02,  6.21it/s]\u001b[A\n",
      " 60%|██████    | 24/40 [00:03<00:02,  6.21it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:03<00:02,  6.21it/s]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:04<00:02,  6.22it/s]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:04<00:02,  6.21it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:04<00:01,  6.19it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:04<00:01,  6.18it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:04<00:01,  6.20it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:04<00:01,  6.21it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:04<00:01,  6.19it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:05<00:01,  6.20it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:05<00:00,  6.20it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:05<00:00,  6.20it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:05<00:00,  6.23it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:05<00:00,  6.21it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:05<00:00,  6.18it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:06<00:00,  6.43it/s]\u001b[A\n",
      "                                                 A\n",
      " 67%|██████▋   | 156/234 [01:32<00:33,  2.30it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.59it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19568416476249695, 'eval_runtime': 6.3726, 'eval_samples_per_second': 392.305, 'eval_steps_per_second': 6.277, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 234/234 [02:08<00:00,  2.30it/s]Saving model checkpoint to finetune_rugec_msu/checkpoint-234\n",
      "Configuration saved in finetune_rugec_msu/checkpoint-234/config.json\n",
      "Configuration saved in finetune_rugec_msu/checkpoint-234/generation_config.json\n",
      "Model weights saved in finetune_rugec_msu/checkpoint-234/model.safetensors\n",
      "tokenizer config file saved in finetune_rugec_msu/checkpoint-234/tokenizer_config.json\n",
      "Special tokens file saved in finetune_rugec_msu/checkpoint-234/special_tokens_map.json\n",
      "Copy vocab file to finetune_rugec_msu/checkpoint-234/spiece.model\n",
      "tokenizer config file saved in finetune_rugec_msu/tokenizer_config.json\n",
      "Special tokens file saved in finetune_rugec_msu/special_tokens_map.json\n",
      "Copy vocab file to finetune_rugec_msu/spiece.model\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: correct_sent, corrupt_sent. If correct_sent, corrupt_sent are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2500\n",
      "  Batch size = 64\n",
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 2/40 [00:00<00:03, 11.61it/s]\u001b[A\n",
      " 10%|█         | 4/40 [00:00<00:04,  7.79it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:00<00:05,  6.85it/s]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:00<00:05,  6.59it/s]\u001b[A\n",
      " 18%|█▊        | 7/40 [00:00<00:05,  6.59it/s]\u001b[A\n",
      " 20%|██        | 8/40 [00:01<00:04,  6.53it/s]\u001b[A\n",
      " 22%|██▎       | 9/40 [00:01<00:04,  6.32it/s]\u001b[A\n",
      " 25%|██▌       | 10/40 [00:01<00:04,  6.11it/s]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:01<00:04,  6.41it/s]\u001b[A\n",
      " 30%|███       | 12/40 [00:01<00:04,  6.31it/s]\u001b[A\n",
      " 32%|███▎      | 13/40 [00:01<00:04,  6.17it/s]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:02<00:04,  6.23it/s]\u001b[A\n",
      " 38%|███▊      | 15/40 [00:02<00:03,  6.29it/s]\u001b[A\n",
      " 40%|████      | 16/40 [00:02<00:03,  6.21it/s]\u001b[A\n",
      " 42%|████▎     | 17/40 [00:02<00:03,  6.26it/s]\u001b[A\n",
      " 45%|████▌     | 18/40 [00:02<00:03,  5.90it/s]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:02<00:03,  5.98it/s]\u001b[A\n",
      " 50%|█████     | 20/40 [00:03<00:03,  6.00it/s]\u001b[A\n",
      " 52%|█████▎    | 21/40 [00:03<00:02,  6.36it/s]\u001b[A\n",
      " 55%|█████▌    | 22/40 [00:03<00:02,  6.30it/s]\u001b[A\n",
      " 57%|█████▊    | 23/40 [00:03<00:02,  6.27it/s]\u001b[A\n",
      " 60%|██████    | 24/40 [00:03<00:02,  6.09it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:03<00:02,  6.19it/s]\u001b[A\n",
      " 65%|██████▌   | 26/40 [00:04<00:02,  6.31it/s]\u001b[A\n",
      " 68%|██████▊   | 27/40 [00:04<00:02,  6.21it/s]\u001b[A\n",
      " 70%|███████   | 28/40 [00:04<00:01,  6.22it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:04<00:01,  6.07it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:04<00:01,  6.30it/s]\u001b[A\n",
      " 78%|███████▊  | 31/40 [00:04<00:01,  6.29it/s]\u001b[A\n",
      " 80%|████████  | 32/40 [00:05<00:01,  6.08it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:05<00:01,  6.27it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:05<00:01,  5.83it/s]\u001b[A\n",
      " 88%|████████▊ | 35/40 [00:05<00:00,  6.05it/s]\u001b[A\n",
      " 90%|█████████ | 36/40 [00:05<00:00,  6.05it/s]\u001b[A\n",
      " 92%|█████████▎| 37/40 [00:05<00:00,  6.04it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:06<00:00,  6.10it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:06<00:00,  6.61it/s]\u001b[A\n",
      "                                                 A\n",
      "100%|██████████| 234/234 [03:36<00:00,  2.30it/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.79it/s]\u001b[A\n",
      "                                               \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 234/234 [03:36<00:00,  1.08it/s]\n",
      "Waiting for the current checkpoint push to be finished, this might take a couple of minutes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19259853661060333, 'eval_runtime': 6.4272, 'eval_samples_per_second': 388.97, 'eval_steps_per_second': 6.224, 'epoch': 3.0}\n",
      "{'train_runtime': 218.5302, 'train_samples_per_second': 68.366, 'train_steps_per_second': 1.071, 'train_loss': 0.20018367278270233, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=234, training_loss=0.20018367278270233, metrics={'train_runtime': 218.5302, 'train_samples_per_second': 68.366, 'train_steps_per_second': 1.071, 'total_flos': 2274457721241600.0, 'train_loss': 0.20018367278270233, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"finetune_rugec_msu\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    resume_from_checkpoint=\"last-checkpoint\",\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.18,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"checkpoint\",\n",
    "    report_to = 'all'\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=dev,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "transformers.logging.set_verbosity_info()\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10d8689c-a456-453b-b562-fd6d7c4a30e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:43:19.299098Z",
     "iopub.status.busy": "2024-12-17T15:43:19.298102Z",
     "iopub.status.idle": "2024-12-17T15:43:31.981810Z",
     "shell.execute_reply": "2024-12-17T15:43:31.980608Z",
     "shell.execute_reply.started": "2024-12-17T15:43:19.299062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: correct_sent, corrupt_sent. If correct_sent, corrupt_sent are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 64\n",
      "100%|██████████| 79/79 [00:12<00:00,  6.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2251938432455063,\n",
       " 'eval_runtime': 12.6494,\n",
       " 'eval_samples_per_second': 395.277,\n",
       " 'eval_steps_per_second': 6.245,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "812c1fad-90bd-4438-8f54-056b80dcb955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:44:21.130608Z",
     "iopub.status.busy": "2024-12-17T15:44:21.129087Z",
     "iopub.status.idle": "2024-12-17T15:44:32.954938Z",
     "shell.execute_reply": "2024-12-17T15:44:32.953571Z",
     "shell.execute_reply.started": "2024-12-17T15:44:21.130557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetune_rugec_msu\n",
      "Configuration saved in finetune_rugec_msu/config.json\n",
      "Configuration saved in finetune_rugec_msu/generation_config.json\n",
      "Model weights saved in finetune_rugec_msu/model.safetensors\n",
      "tokenizer config file saved in finetune_rugec_msu/tokenizer_config.json\n",
      "Special tokens file saved in finetune_rugec_msu/special_tokens_map.json\n",
      "Copy vocab file to finetune_rugec_msu/spiece.model\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}}\n",
      "events.out.tfevents.1734450211.l-6413647b-b198-4a9a-b7d3-3bd9d1634e63.2885.1: 100%|██████████| 359/359 [00:00<00:00, 1.72kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mika5883/finetune_rugec_msu/commit/5beb8f7153022f66d39b3696c4092a5cd88533d6', commit_message='finetuned_msu', commit_description='', oid='5beb8f7153022f66d39b3696c4092a5cd88533d6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mika5883/finetune_rugec_msu', endpoint='https://huggingface.co', repo_type='model', repo_id='mika5883/finetune_rugec_msu'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub('finetuned_msu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919b245-a581-4780-90c4-97345f3fe7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
